{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bN6YH_m2xnP7",
        "UAexU2TUyIk7",
        "1u9LaiR_CKFf",
        "waIUf-v2wzTF",
        "6nQu1bSQuSoh",
        "wkoNeK8G9xxA",
        "bFAH5t3X8daN",
        "IZQEv1RgFnQn",
        "1BuiLobUFp_O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63a584c00cbd4a7ab94c78d66e246407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2e35113d99e46bb81d6f2d892bf902f",
              "IPY_MODEL_728c64ced56b4fb38437c1e7af292fdb",
              "IPY_MODEL_a7507e7dd0c942128d6ae303b6b0ee55"
            ],
            "layout": "IPY_MODEL_a823f24d7d4241afb4fdd348c1116593"
          }
        },
        "f2e35113d99e46bb81d6f2d892bf902f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f39ca9d58443eebdba964baa2fbf3c",
            "placeholder": "​",
            "style": "IPY_MODEL_9b359769e7104ab2998074861370627f",
            "value": "100%"
          }
        },
        "728c64ced56b4fb38437c1e7af292fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b49a64dd1e428990a88625e7e58b2a",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60fcee80f02b46f4bbe1018add4e3d40",
            "value": 167502836
          }
        },
        "a7507e7dd0c942128d6ae303b6b0ee55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7b11ad1be704bb1ad18b77f314345a6",
            "placeholder": "​",
            "style": "IPY_MODEL_9682c5b974274494af0ce1a5f4db04b0",
            "value": " 160M/160M [00:00&lt;00:00, 205MB/s]"
          }
        },
        "a823f24d7d4241afb4fdd348c1116593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f39ca9d58443eebdba964baa2fbf3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b359769e7104ab2998074861370627f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9b49a64dd1e428990a88625e7e58b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60fcee80f02b46f4bbe1018add4e3d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7b11ad1be704bb1ad18b77f314345a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9682c5b974274494af0ce1a5f4db04b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Image Narration\n",
        "\n",
        "This notebook can be used to load an existing trained model, and apply it to images. The vision model is pretrained by FiftyOne, and the natural language generation model is selected from the saved copies of models trained by the Model Trainer notebook."
      ],
      "metadata": {
        "id": "NQzEm77BguTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this notebook must be configured to read from a drive folder, which needs to contain the supporting code found in the repository."
      ],
      "metadata": {
        "id": "E6AMm5yKxtHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach - Two-Step Pipeline Model"
      ],
      "metadata": {
        "id": "bN6YH_m2xnP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and setup"
      ],
      "metadata": {
        "id": "UAexU2TUyIk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone"
      ],
      "metadata": {
        "id": "DC-3gwpdyK3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2c161d-7c4b-448c-ab3f-68ac75d6223e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.18.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 11.9 MB/s \n",
            "\u001b[?25hCollecting Jinja2>=3\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting fiftyone-brain<0.10,>=0.9.2\n",
            "  Downloading fiftyone_brain-0.9.2-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting eventlet\n",
            "  Downloading eventlet-0.33.2-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 20.9 MB/s \n",
            "\u001b[?25hCollecting pprintpp\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from fiftyone) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fiftyone) (21.3)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9 MB 106 kB/s \n",
            "\u001b[?25hCollecting hypercorn>=0.13.2\n",
            "  Downloading Hypercorn-0.14.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 668 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from fiftyone) (0.18.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.28-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting Deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fiftyone) (0.8.10)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from fiftyone) (2022.6)\n",
            "Collecting voxel51-eta<0.9,>=0.8.1\n",
            "  Downloading voxel51_eta-0.8.1-py2.py3-none-any.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 41.3 MB/s \n",
            "\u001b[?25hCollecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting strawberry-graphql==0.138.1\n",
            "  Downloading strawberry_graphql-0.138.1-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 36.1 MB/s \n",
            "\u001b[?25hCollecting ndjson\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fiftyone) (1.21.6)\n",
            "Requirement already satisfied: pymongo>=3.11 in /usr/local/lib/python3.8/dist-packages (from fiftyone) (4.3.3)\n",
            "Collecting sse-starlette<1,>=0.10.3\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from fiftyone) (57.4.0)\n",
            "Collecting sseclient-py<2,>=1.7.2\n",
            "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting mongoengine==0.24.2\n",
            "  Downloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting motor>=2.3\n",
            "  Downloading motor-3.1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (from fiftyone) (4.6.0.66)\n",
            "Collecting fiftyone-db<0.5,>=0.4\n",
            "  Downloading fiftyone_db-0.4.0-py3-none-manylinux1_x86_64.whl (37.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.8 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.8/dist-packages (from fiftyone) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from fiftyone) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fiftyone) (1.3.5)\n",
            "Collecting starlette==0.20.4\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 925 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fiftyone) (1.0.2)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.8/dist-packages (from fiftyone) (5.5.0)\n",
            "Collecting argcomplete\n",
            "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from fiftyone) (5.4.8)\n",
            "Collecting dacite>=1.6.0\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fiftyone) (3.2.2)\n",
            "Collecting retrying\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.20.4->fiftyone) (4.4.0)\n",
            "Collecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting graphql-core<3.3.0,>=3.2.0\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 21.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fiftyone) (2.10)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from fiftyone-brain<0.10,>=0.9.2->fiftyone) (1.7.3)\n",
            "Collecting wsproto>=0.14.0\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting h11\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.10.2)\n",
            "Collecting h2>=3.1.0\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting priority\n",
            "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Collecting hpack<5,>=4.0\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting hyperframe<7,>=6.0\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=3->fiftyone) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.14->fiftyone) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.14->fiftyone) (8.1.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from pymongo>=3.11->fiftyone) (2.2.1)\n",
            "Collecting httpx>=0.10.0\n",
            "  Downloading httpx-0.23.1-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2022.9.24)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.2-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (1.5.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (1.24.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (0.3.6)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (2.23.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.28\n",
            "  Downloading botocore-1.29.28-py3-none-any.whl (10.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 33.5 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
            "Requirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.8/dist-packages (from eventlet->fiftyone) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fiftyone) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fiftyone) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fiftyone) (3.0.9)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->voxel51-eta<0.9,>=0.8.1->fiftyone) (3.0.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (2.8.8)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (2022.10.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fiftyone) (3.1.0)\n",
            "Installing collected packages: sniffio, urllib3, rfc3986, jmespath, h11, anyio, hyperframe, httpcore, hpack, botocore, wsproto, starlette, s3transfer, retrying, priority, patool, ndjson, httpx, h2, graphql-core, argcomplete, xmltodict, voxel51-eta, universal-analytics-python3, strawberry-graphql, sseclient-py, sse-starlette, pprintpp, motor, mongoengine, kaleido, Jinja2, hypercorn, fiftyone-db, fiftyone-brain, eventlet, Deprecated, dacite, boto3, aiofiles, fiftyone\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.13 Jinja2-3.1.2 aiofiles-22.1.0 anyio-3.6.2 argcomplete-2.0.0 boto3-1.26.28 botocore-1.29.28 dacite-1.6.0 eventlet-0.33.2 fiftyone-0.18.0 fiftyone-brain-0.9.2 fiftyone-db-0.4.0 graphql-core-3.2.3 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.16.2 httpx-0.23.1 hypercorn-0.14.3 hyperframe-6.0.1 jmespath-1.0.1 kaleido-0.2.1 mongoengine-0.24.2 motor-3.1.1 ndjson-0.3.1 patool-1.12 pprintpp-0.4.0 priority-2.0.0 retrying-1.3.4 rfc3986-1.5.0 s3transfer-0.6.0 sniffio-1.3.0 sse-starlette-0.10.3 sseclient-py-1.7.2 starlette-0.20.4 strawberry-graphql-0.138.1 universal-analytics-python3-1.1.1 urllib3-1.25.11 voxel51-eta-0.8.1 wsproto-1.2.0 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "0H_XBru453nS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de85252e-7134-463f-97d7-6f4c9ef324ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as func\n",
        "from fiftyone import ViewField as F\n",
        "import random"
      ],
      "metadata": {
        "id": "2xLeNqzo_AA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd682f1b-e56f-4c85-b758-0fc020486aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Migrating database to v0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update this section to connect to your google drive:**"
      ],
      "metadata": {
        "id": "5VxZUzBMx34w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "content_path = '/content/drive/MyDrive/4 - Senior Year/First Semester/COMP 484/Project/CSCAPSTONE'\n",
        "%cd '/content/drive/MyDrive/4 - Senior Year/First Semester/COMP 484/Project/CSCAPSTONE'\n",
        "# content_path = 'drive/MyDrive/CSCAPSTONE'\n",
        "# %cd 'drive/MyDrive/CSCAPSTONE'\n",
        "\n",
        "# code dependencies (Text.py, LSTM_class.py, etc.) need to be in directory of content_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg1mGIrnKktr",
        "outputId": "0ad73f0a-bc4a-4600-ebbc-fbb7479ee630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1xawm5SPBJkiVQVQP5PI5uwYDs18SeI_N/CSCAPSTONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision Component"
      ],
      "metadata": {
        "id": "1u9LaiR_CKFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FiftyOne data and model"
      ],
      "metadata": {
        "id": "waIUf-v2wzTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvskOCyFp6vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef8f4cd-ab97-4d56-c5ca-4ba3da1d6acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████|    1.9Gb/1.9Gb [8.9s elapsed, 0s remaining, 214.1Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    1.9Gb/1.9Gb [8.9s elapsed, 0s remaining, 214.1Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████|    6.1Gb/6.1Gb [24.6s elapsed, 0s remaining, 261.5Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    6.1Gb/6.1Gb [24.6s elapsed, 0s remaining, 261.5Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 5000/5000 [39.2s elapsed, 0s remaining, 139.4 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 5000/5000 [39.2s elapsed, 0s remaining, 139.4 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'detector-recipe' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'detector-recipe' created\n"
          ]
        }
      ],
      "source": [
        "### grab data\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    dataset_name=\"detector-recipe\",\n",
        ")\n",
        "\n",
        "classes = dataset.default_classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on GPU if it is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a pre-trained Faster R-CNN model\n",
        "vision_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# vision_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=Trueweights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
        "vision_model.to(device)\n",
        "vision_model.eval()\n",
        "\n",
        "print(\"vision_model ready\")"
      ],
      "metadata": {
        "id": "ABUGcoYMyUbW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "63a584c00cbd4a7ab94c78d66e246407",
            "f2e35113d99e46bb81d6f2d892bf902f",
            "728c64ced56b4fb38437c1e7af292fdb",
            "a7507e7dd0c942128d6ae303b6b0ee55",
            "a823f24d7d4241afb4fdd348c1116593",
            "b9f39ca9d58443eebdba964baa2fbf3c",
            "9b359769e7104ab2998074861370627f",
            "e9b49a64dd1e428990a88625e7e58b2a",
            "60fcee80f02b46f4bbe1018add4e3d40",
            "c7b11ad1be704bb1ad18b77f314345a6",
            "9682c5b974274494af0ce1a5f4db04b0"
          ]
        },
        "outputId": "db2a948f-d10c-4566-9958-7362833296d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a584c00cbd4a7ab94c78d66e246407"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vision_model ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Formatting and interoperability code"
      ],
      "metadata": {
        "id": "6nQu1bSQuSoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vision_predict_sample(sample, model):\n",
        "  \"\"\"Code decomposed from standard FiftyOne computer vision tutorial. Takes a\n",
        "  sample from a FiftyOne view, and uses a computer vision model to make a\n",
        "  prediction of what objects are present in the image.\"\"\"\n",
        "  # Load image\n",
        "  image = Image.open(sample.filepath)\n",
        "  image = func.to_tensor(image).to(device)\n",
        "  c, h, w = image.shape\n",
        "  # Perform inference\n",
        "  preds = model([image])[0]\n",
        "  labels = preds[\"labels\"].cpu().detach().numpy()\n",
        "  scores = preds[\"scores\"].cpu().detach().numpy()\n",
        "  boxes = preds[\"boxes\"].cpu().detach().numpy()\n",
        "  # Convert detections to FiftyOne format\n",
        "  detections = []\n",
        "  for label, score, box in zip(labels, scores, boxes):\n",
        "    # Convert to [top-left-x, top-left-y, width, height]\n",
        "    # in relative coordinates in [0, 1] x [0, 1]\n",
        "    x1, y1, x2, y2 = box\n",
        "    rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
        "    detections.append(\n",
        "      fo.Detection(\n",
        "        label=classes[label],\n",
        "        bounding_box=rel_box,\n",
        "        confidence=score\n",
        "      )\n",
        "    )\n",
        "  # Save predictions to dataset\n",
        "  sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "  sample.save()"
      ],
      "metadata": {
        "id": "74NxUrSP6g7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_to_onehot(sample, model, conf_thresh=0.50):\n",
        "  \"\"\"Accepts a sample as input, and returns a one-hot vector representing the\n",
        "  classes detected in that sample image as an output. If predictions have not\n",
        "  already been made for this sample, then predictions are made.\n",
        "  Should return a one-hot vector, containing 1s for each class of object\n",
        "  that was detected. The 0-th position of the vector should always be 0 (this \n",
        "  is to match the convention that the class list, `classes`, always reserves\n",
        "  the index 0).\"\"\"\n",
        "  vec = [0 for i in range(0, len(classes))]\n",
        "  if not hasattr(sample, \"predictions\") or sample.predictions is None:\n",
        "    vision_predict_sample(sample, model)\n",
        "  predictions = sample.predictions\n",
        "  for detection in predictions.detections:\n",
        "    if detection['confidence'] > conf_thresh:\n",
        "      class_str = detection['label']\n",
        "      onehot_index = classes.index(class_str)\n",
        "      vec[onehot_index] = 1\n",
        "  # TODO: determine whether this information is easier to use as a list or a string\n",
        "  # one_hot = vec\n",
        "  one_hot = ''.join(map(str,vec))\n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "H30YM9KEbi5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_onehot(sample, model, conf_thresh=0.5):\n",
        "  sample[\"one_hot\"] = sample_to_onehot(sample, vision_model)\n",
        "  sample.save()"
      ],
      "metadata": {
        "id": "LVOu5RvTBYjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_to_labels(one_hot, classes):\n",
        "  labels = []\n",
        "  for i in range(len(one_hot)):\n",
        "    if int(one_hot[i]) == 1:\n",
        "      labels.append(classes[i])\n",
        "  return labels"
      ],
      "metadata": {
        "id": "u9VvLo3Y_IN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLG Component"
      ],
      "metadata": {
        "id": "wkoNeK8G9xxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These three imports refer to functions in files from the mounted google drive.\n",
        "import functions as f\n",
        "from Text import *\n",
        "from LSTM_class import *\n",
        "\n",
        "from keras import layers, models, optimizers\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "F6oBwRY-qs8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### preprocessing"
      ],
      "metadata": {
        "id": "fm1s0uKZIA-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get and prepare training data\n",
        "path_train = content_path + '/data/train.txt'\n",
        "input_train = f.read_txt(path_train)"
      ],
      "metadata": {
        "id": "J1BCoR3sAYeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two training sets from the same corpus, one containing every word of the corpus in the order they were written, and another containing all of the words of the corpus in reverse order."
      ],
      "metadata": {
        "id": "98T_W6JehvHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 4\n",
        "step = 3\n",
        "\n",
        "text_train_forward = Text(input_train, reverse=False)\n",
        "text_train_reverse = Text(input_train, reverse=True)\n",
        "text_train_forward.tokens_info()\n",
        "\n",
        "seq_train_forward = Sequences(text_train_forward, max_len, step)\n",
        "seq_train_reverse = Sequences(text_train_reverse, max_len, step)\n",
        "seq_train_forward.sequences_info()"
      ],
      "metadata": {
        "id": "CC3saYjfA4GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c9b061-1579-4908-d5e4-438934c1c64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total tokens: 1428900, distinct tokens: 42415\n",
            "number of sequences of length 4: 476299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some examples of sequences in the corpus:"
      ],
      "metadata": {
        "id": "JWfvGfnBiPfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_train_forward.tokens[:10])\n",
        "print(text_train_forward.tokens_ind[:10], '\\n')\n",
        "np.array(seq_train_forward.sequences[:3])"
      ],
      "metadata": {
        "id": "KRzQ3iO7HGJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3505821e-02d1-4ec3-8b6c-c08c485052d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'entered', 'this', 'incarnation', 'on', 'March', 'the', 'twenty', '-', 'ninth']\n",
            "[8273, 13751, 37818, 19540, 2093, 22253, 7033, 26673, 36328, 31989] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8273, 13751, 37818, 19540],\n",
              "       [19540,  2093, 22253,  7033],\n",
              "       [ 7033, 26673, 36328, 31989]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reverse sequences are not necessarily exact reverses of the forward sequences because the total number of tokens in the corpus doesn't necessariy divide evenly into 4-word subsequences, so one to three words may be left off of the end."
      ],
      "metadata": {
        "id": "lpIqWfADRkG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_train_reverse.tokens[-10:])\n",
        "print(text_train_reverse.tokens_ind[-10:], '\\n')\n",
        "np.array(seq_train_reverse.sequences[-3:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddPdlOsnQVPQ",
        "outputId": "b4b48205-7f5b-4f60-d679-57c25508b045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ninth', '-', 'twenty', 'the', 'March', 'on', 'incarnation', 'this', 'entered', 'I']\n",
            "[31989, 36328, 26673, 7034, 22253, 2093, 19540, 37818, 13751, 8274] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19677,  9512, 31989, 36328],\n",
              "       [36328, 26673,  7034, 22253],\n",
              "       [22253,  2093, 19540, 37818]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for the model to work well, we want to ensure that at least most of the image keywords that we'll be asking it to narrate are present in the vocabulary of the natural language model. This code tells us which keywords are present and which are missing:"
      ],
      "metadata": {
        "id": "kxDoImLqh409"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_in_corpus(keyword, corpus=text_train_forward):\n",
        "  subwords = keyword.split(' ')  # some COCO keywords are actually two words\n",
        "  # TODO: two digit numbers should also be considered as two individual digits?\n",
        "  flag = True\n",
        "  for subword in subwords:\n",
        "    flag = flag and subword in corpus.token2ind.keys()\n",
        "  return flag\n",
        "\n",
        "def validate_corpus(corpus):\n",
        "  \"\"\"Returns a list of any tokens which might be detected in an image by the\n",
        "  vision model, but which are not in the vocabulary of this corpus. Ideally,\n",
        "  this list should only contain the number 0.\"\"\"\n",
        "  missing_vocab = []\n",
        "  present_vocab = []\n",
        "  for word in classes:\n",
        "    l = present_vocab if keyword_in_corpus(word, corpus) else missing_vocab\n",
        "    l.append(word)\n",
        "  return {'missing':missing_vocab, 'present':present_vocab}"
      ],
      "metadata": {
        "id": "kzfmehuys5Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attendance = validate_corpus(text_train_forward)\n",
        "print(\"corpus contains\", len(attendance['present']), \"MSCOCO keywords, out of\", len(classes), \"--- this is about\", int(10000*(len(attendance['present'])/len(classes)))/100, \"% attendance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_T1JAgyti0U",
        "outputId": "d1e5777b-7be8-4d73-baa6-f2b8c964a6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus contains 70 MSCOCO keywords, out of 91 --- this is about 76.92 % attendance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attendance['missing'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVocwWXgu0IO",
        "outputId": "78735ecd-1d2c-4475-dd34-16e77953f469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['motorcycle', 'airplane', 'fire hydrant', 'zebra', 'giraffe', 'backpack', 'frisbee', 'skis', 'snowboard', 'kite', 'skateboard', 'surfboard', 'broccoli', 'pizza', 'donut', 'tv', 'laptop', 'keyboard', 'microwave', 'toaster', 'teddy bear']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're missing a few modern words, but we still have 70 out of 91 keywords available, so the language model should have plenty to say for every image, even if it doesn't recognize a few keywords here and there."
      ],
      "metadata": {
        "id": "ThHSs3bLiUoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading natural language model"
      ],
      "metadata": {
        "id": "C-HH5_V7IC4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use keras to load an existing natural language model. We also need to lean on a bit of json, because even with the same corpus, different models sometimes encode text differently. For example, one model might associate the word token \"person\" with the index `18382`, while another model might associate \"person\" with index `918`. In order to reuse a model, we have to know how it maps words to these indices, and that's what we store as json."
      ],
      "metadata": {
        "id": "ADagPIKZieGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "Qd8bMwEh2hmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_nano = \"1670852439439527500\"  # chooses a particular version of the trained models, based on the nano time that the forward model finished training\n",
        "# this particular nano corresponds to a model which was trained for 90 minutes.\n",
        "# loads the version of the token--index mappings that this particular model was trained on\n",
        "version_path = content_path + \"/out/model_\" + str(target_nano) + \"/\"\n",
        "token2ind_forward = json.load(open(version_path + \"word_mapping_forward.json\"))\n",
        "ind2token_forward = {value: key for (key, value) in token2ind_forward.items()}\n",
        "map_forward = (token2ind_forward, ind2token_forward)\n",
        "token2ind_reverse = json.load(open(version_path + \"word_mapping_reverse.json\"))\n",
        "ind2token_reverse = {value: key for (key, value) in token2ind_reverse.items()}\n",
        "map_reverse = (token2ind_reverse, ind2token_reverse)\n",
        "model_nlg_forward = models.load_model(version_path + \"model_nlg_forward\")\n",
        "model_nlg_reverse = models.load_model(version_path + \"model_nlg_reverse\")"
      ],
      "metadata": {
        "id": "ak9zbcbqupo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Natural language generation code"
      ],
      "metadata": {
        "id": "KGa4IzJ8S_4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code which performs the basic NLG tasks required by the model. This doesn't interact with or account for images in any way, this code only accomplishes the step of generating sentences from keywords."
      ],
      "metadata": {
        "id": "_U7dkNATjHdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def produce_reversed_prediction(pred, text):\n",
        "  r0 = text.split(' ')\n",
        "  r0.reverse()\n",
        "  r0 = ' '.join(r0)\n",
        "  return pred.reverse_preprocess(r0)"
      ],
      "metadata": {
        "id": "kZm6tpWreiNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reversed_prediction(pred, length=100, temperature=1):\n",
        "  text = pred.generate_sequence(length, temperature=temperature, ret_unprocessed=True)\n",
        "  return produce_reversed_prediction(pred, text)"
      ],
      "metadata": {
        "id": "xMBpf2Lki8jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentences(string):\n",
        "  \"\"\"Splits an input containing multiple sentences into a list of seperate\n",
        "  sentences, including the original punctuation.\"\"\"\n",
        "  # please god do not look at this dispicable regex\n",
        "  delimiters = '.!?'\n",
        "  l = []\n",
        "  l.append([])\n",
        "  for c in string:\n",
        "    l[-1].append(c)\n",
        "    if c in delimiters:\n",
        "      l.append([])\n",
        "  l2 = []\n",
        "  for slist in l:\n",
        "    s_l = ''.join(slist)\n",
        "    if not (s_l == '' or s_l == ' '):  # iffy attempt to address trailing whitespace\n",
        "      l2.append(s_l.strip())\n",
        "  return l2"
      ],
      "metadata": {
        "id": "VPazAIPzfOvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_descriptive_sentence(keyword, seq_len=100, temperature=1):\n",
        "  \"\"\"Given an input keyword contained in the corpus, generates a single sentence\n",
        "  of first-person English natural language which includes that keyword.\n",
        "  `seq_len` determines the maximum length of the subsequences generated when\n",
        "  generating the full target sentence. This is not necessarily the length of the\n",
        "  resulting sentence---typically, that sentence will be shorter, because only\n",
        "  one complete sentence from the final output is returned. The final output\n",
        "  will be no more than `2 * seq_len` tokens long (where our tokens are words and\n",
        "  punctuation). Low `seq_len` can occasionally result in truncated sentences,\n",
        "  wherein the model did not generate a full sentence before it reached the token\n",
        "  limit. High `seq_len`, however, is more computationally expensive for equal\n",
        "  outputs, because most of the additional tokens are not kept in the final\n",
        "  output. For this reason, a sequence length of around 50-150 is recommended.\n",
        "  `temperature` is a randomization factor between 0 and 1. The higher the\n",
        "  temperature, the more likely the model is to use unusual words from its corpus\n",
        "  (essentially, the more variety there will be in the output). Lower temperature\n",
        "  results in very stable and more gramatically correct language, but much less\n",
        "  variation and 'color' in the output.\"\"\"\n",
        "  # known issues:\n",
        "  #    sometimes produces really long sentences. Maybe reduce seq_len?\n",
        "  #    this may sometimes falsely cut sentence borders at Mr. or Ms. and other such abbreviations that include periods\n",
        "  # \n",
        "  # if the keyword is actually two or more words, such as \"wine glass\"\n",
        "  # then reverse it, so it comes out correctly \n",
        "  reverse_keyword = keyword if len(keyword.split(' ')) == 1 else ' '.join(reversed(keyword.split(' ')))\n",
        "  pred_reverse = gen_pred(reverse_keyword, model_nlg_reverse, *map_reverse)\n",
        "  reverse_prediction = generate_reversed_prediction(pred_reverse, seq_len, temperature)\n",
        "  last_sentence_of_reverse = split_sentences(reverse_prediction)[-1]\n",
        "  pred_forward = gen_pred(last_sentence_of_reverse, model_nlg_forward, *map_forward)\n",
        "  forward_prediction = pred_forward.generate_sequence(seq_len, temperature=temperature)\n",
        "  first_sentence_of_forward = split_sentences(forward_prediction)[0]\n",
        "  # adds punctuation if the sentence was cut off early\n",
        "  if first_sentence_of_forward[-1] not in \".!?\":\n",
        "    first_sentence_of_forward += \".\"\n",
        "  return first_sentence_of_forward\n",
        "\n",
        "\n",
        "def gen_pred(prefix, model, token2ind, ind2token):\n",
        "  \"\"\"Convenience function that wraps the process of making a ModelPredict\n",
        "  object.\"\"\"\n",
        "  text_prefix= Text(prefix, token2ind, ind2token)\n",
        "  return ModelPredict(model, text_prefix, token2ind, ind2token, max_len, embedding=True)"
      ],
      "metadata": {
        "id": "zAW9DSQAgmsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synthesis"
      ],
      "metadata": {
        "id": "bFAH5t3X8daN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Synthesis code"
      ],
      "metadata": {
        "id": "IZQEv1RgFnQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code interoperates between the vision model and the natural language model, and provides functions for producing natural language paragraphs from input images. The main function here is `narrate_images()`"
      ],
      "metadata": {
        "id": "HCxFdioyj4BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_classes_in(samples_view, vision_model, conf_thresh=0.50):\n",
        "  class_detections = []\n",
        "  for sample in samples_view:\n",
        "    generate_onehot(sample, vision_model, conf_thresh)\n",
        "    labels = onehot_to_labels(sample['one_hot'], classes)\n",
        "    for label in labels:  # but appending individual labels lets us easily filter out to only unique labels\n",
        "      class_detections.append(label)\n",
        "  return tuple(set(class_detections))  # removes duplicate labels"
      ],
      "metadata": {
        "id": "alJJNnBDA2bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences_from(class_detections, temperature=0.7, max_sentences=3, seed=None):\n",
        "  # randomly reorders the keywords, so their corresponding sentences don't just\n",
        "  # appear in alphabetical order for every prediction.\n",
        "  if seed is None:\n",
        "    keywords = random.sample(class_detections, len(class_detections))\n",
        "  else:\n",
        "    random.seed(seed)\n",
        "    keywords = random.sample(class_detections, len(class_detections))\n",
        "  # only generates up to `max_sentences` sentences. This means that, in cases\n",
        "  # where there are many detections, some may be ignored. This process could\n",
        "  # be improved by, perhaps, prioritizing high confidence detections over low\n",
        "  # confidence detections, when deciding which to include and which to exclude.\n",
        "  sentences = []\n",
        "  for keyword in keywords:\n",
        "    if len(sentences) < max_sentences and keyword_in_corpus(keyword):\n",
        "      sentences.append(generate_descriptive_sentence(keyword, temperature=temperature))\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "R3XplVdlDa6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def narrate_images(samples_view, conf_thresh=0.5, temperature=0.7, max_sentences=3, seed=None):\n",
        "  \"\"\"Given a set of input images, generates first-person narrative text based on\n",
        "  the detected contents of those images.\n",
        "  `conf_thresh` determines how confident the vision model must be in a detection\n",
        "  in order for it to be included in the list of classes which the natural\n",
        "  language model generates sentences for. If this is very high, some images\n",
        "  might not result in any output.\n",
        "  `temperature` determines how much the natural language model will deviate from\n",
        "  the most likely word selection. If this value is high, then the generate text\n",
        "  will almost always be gramatically correct, but is likely to be repetitive. If\n",
        "  the temperature is low, then the output will be much more varied, but it may\n",
        "  become ungrammatical, and is more likely to be nonsensical.\n",
        "  `max_sentences` is the maximum number of complete sentences which the natural\n",
        "  language model will attempt generate from the given set of images. If this is\n",
        "  high, the output could be very long, but will almost certainly include a\n",
        "  sentence for every detected class of object. If this value is low, the output\n",
        "  will be shorter and more concise, but may leave out some of the detections.\n",
        "  `seed` is a seed used for random processes, namely the random reordering of\n",
        "  keywords. This won't cause the same text to be generated on subsequent calls\n",
        "  (randomness is baked into the natural language model), but it will cause the\n",
        "  class detections to be presented in the same order each time.\"\"\"\n",
        "  class_detections = find_classes_in(samples_view, vision_model, conf_thresh)\n",
        "  sentences = generate_sentences_from(class_detections, temperature, max_sentences, seed)\n",
        "  paragraph = ' '.join(sentences)\n",
        "  return paragraph"
      ],
      "metadata": {
        "id": "lgBEsiTlEuy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def narrate_n(num_images, conf_thresh=0.5, temperature=0.7, max_sentences=3, seed=None):\n",
        "  \"\"\"Chooses `num_images` images at random from the currently selected FiftyOne\n",
        "  dataset, and uses them to generate first-person narrative text.\"\"\"\n",
        "  samples_view = dataset.take(num_images) if seed is None else dataset.take(num_images, seed)\n",
        "  paragraph = narrate_images(samples_view, conf_thresh, temperature, max_sentences, seed)\n",
        "  return paragraph"
      ],
      "metadata": {
        "id": "9ifqVkdsFOSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "1BuiLobUFp_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code selects three images from the MSCOCO 2017 dataset, and then applies our code to generate a first-person narrative about the contents of the image. A different set of images can be selected by changing the `seed` variable."
      ],
      "metadata": {
        "id": "HE4Dj31OmzSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 105"
      ],
      "metadata": {
        "id": "ljvT9bc5nAH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_view = dataset.take(3, seed=seed)\n",
        "session = fo.launch_app(samples_view)"
      ],
      "metadata": {
        "id": "0QDrA54RcksO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "0e65a641-b815-429f-d2d9-1433f90c75ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "#focontainer-177ea5fa-0fa8-429d-ab18-73ffeec6adea {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-177ea5fa-0fa8-429d-ab18-73ffeec6adea {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-177ea5fa-0fa8-429d-ab18-73ffeec6adea:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-177ea5fa-0fa8-429d-ab18-73ffeec6adea {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-177ea5fa-0fa8-429d-ab18-73ffeec6adea\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-177ea5fa-0fa8-429d-ab18-73ffeec6adea\">\n",
              "      <button id=\"foactivate-177ea5fa-0fa8-429d-ab18-73ffeec6adea\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# narrate_images(samples_view, seed=seed)"
      ],
      "metadata": {
        "id": "dmXjzLfOdFOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(narrate_images(samples_view, seed=seed))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFp3mCl-N-rt",
        "outputId": "58903eae-e319-4c36-c9af-dd3792ebc119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I saw him, he found it was left, and gave it a story of the dog. I went back to my hold of my time, knowing was, and that the college, but when they came back to me, I suppose, but it were well of all of of the business to my wife, who, her husband and the daughter, a note, forty- five days later in a person, mainly of the-- sage attendants of she was a great a good man. I stood in the window, I looked at him, and straight, and went to the car, and I would have the same room for the night.\n",
            "\n",
            "One of the day or the present it was the dog and stopped the General was the one, as the old age of the-- personal son was was was up. I gave him not a person to take the place, would not school, I was after the third know it was the first time in the number of my first strict and; that the right of the house is not going to the very men who is the where they lived in the house that of their personal intelligence. He came out of the car of the day, being a comfortable and a man.\n",
            "\n",
            "Yet him all the dog was now, and she said, and this building they did not help, he was and one of our time, we still made a little sum, if they were, they are only a one in whom we were had so many other means of interest in the country question, he never only once had for his name that he was he was in of a second day being in the. No, it is, is all good- well, that I will, and I suppose I will take him out of me in the morning, with one- of cloth, and that service, at once, not one person, and if she would go about the work; and I could see that it was to the business, with his wife, I was no one but the other day, and, though he was bearing school for me to come to the six years in London at the night. A sight and Police, and went out of the car where the first three man at the very long.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}